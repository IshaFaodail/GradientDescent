Gradient Descent Variants: Implementing from Scratch vs. Torch Optim for Regression

Guide: Prof. Somdyuti Paul — (Course Project) (Feb’24)

• Implemented custom Adam and NAG optimizers achieving initial training losses of 0.7137 and 0.8152, respectively,
and final losses of 0.1851 and 0.2028 after 100 epochs.

• Compared custom and built-in optimizers, showcasing competitive performance with Adam and NAG initial losses
of 0.7137 vs 6.2741 and 0.8152 vs 5.5048, respectively.

• Custom Adam and NAG optimizers effectively minimized training losses, demonstrating their efficiency in optimizing
neural network parameters for regression tasks.
